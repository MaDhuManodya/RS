{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üí™ FitMatch ‚Äî Workout Plan Recommender System\n",
    "\n",
    "---\n",
    "\n",
    "## Step 1: Problem Definition\n",
    "\n",
    "**Domain:** Fitness / Workout Recommendation\n",
    "\n",
    "**What is being recommended?**  \n",
    "Workout plans (e.g., \"Morning Run Blast\", \"Full Body Strength\") to users based on their fitness profile and past preferences.\n",
    "\n",
    "**Who are the users?**  \n",
    "Fitness enthusiasts with varying ages (18‚Äì59), fitness levels (beginner/intermediate/advanced), and goals (weight loss, muscle gain, endurance, flexibility).\n",
    "\n",
    "**Objective:**  \n",
    "Top-N recommendation (Top-5) ‚Äî for each user, suggest 5 workout plans they are most likely to enjoy.\n",
    "\n",
    "**Algorithms:**\n",
    "1. **Most Popular** (Baseline) ‚Äî Recommend plans with highest average rating\n",
    "2. **User-Based Collaborative Filtering** (Primary) ‚Äî Cosine similarity on user-item rating matrix\n",
    "3. **Content-Based Filtering** (Bonus) ‚Äî TF-IDF on plan descriptions + cosine similarity\n",
    "4. **Hybrid** (Bonus) ‚Äî Weighted blend of CF + Content-Based\n",
    "\n",
    "**Justification:**  \n",
    "User-Based CF is ideal for this domain because users with similar fitness profiles tend to enjoy similar workouts. The content-based approach adds value by capturing workout attribute similarity, and the hybrid model combines the strengths of both."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 2: Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Import our modules\n",
    "from generate_dataset import generate_users, generate_workout_plans, generate_ratings\n",
    "from recommender import (\n",
    "    MostPopularRecommender,\n",
    "    UserBasedCFRecommender,\n",
    "    ContentBasedRecommender,\n",
    "    HybridRecommender,\n",
    "    split_train_test,\n",
    "    evaluate_model,\n",
    "    precision_at_k,\n",
    "    recall_at_k\n",
    ")\n",
    "\n",
    "print('All modules imported successfully!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the dataset\n",
    "np.random.seed(42)\n",
    "\n",
    "users = generate_users(n=100)\n",
    "plans = generate_workout_plans(n=60)\n",
    "ratings = generate_ratings(users, plans)\n",
    "\n",
    "print(f'Users:    {len(users)}')\n",
    "print(f'Plans:    {len(plans)}')\n",
    "print(f'Ratings:  {len(ratings)}')\n",
    "\n",
    "# Sparsity\n",
    "total_possible = len(users) * len(plans)\n",
    "sparsity = 1 - len(ratings) / total_possible\n",
    "print(f'\\nTotal possible interactions: {total_possible}')\n",
    "print(f'Dataset Sparsity: {sparsity:.2%}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore user attributes\n",
    "print('=== User Demographics ===')\n",
    "print(f'\\nAge range: {users[\"age\"].min()} ‚Äì {users[\"age\"].max()}')\n",
    "print(f'Mean age: {users[\"age\"].mean():.1f}')\n",
    "print(f'\\nFitness Level Distribution:')\n",
    "print(users['fitness_level'].value_counts())\n",
    "print(f'\\nGoal Distribution:')\n",
    "print(users['goal'].value_counts())\n",
    "\n",
    "users.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore workout plans\n",
    "print('=== Workout Plans ===')\n",
    "print(f'\\nPlan Types:')\n",
    "print(plans['type'].value_counts())\n",
    "print(f'\\nDifficulty Distribution:')\n",
    "print(plans['difficulty'].value_counts())\n",
    "print(f'\\nTarget Goals:')\n",
    "print(plans['target_goal'].value_counts())\n",
    "print(f'\\nDuration range: {plans[\"duration_min\"].min()} ‚Äì {plans[\"duration_min\"].max()} minutes')\n",
    "\n",
    "plans[['plan_id', 'name', 'type', 'difficulty', 'duration_min', 'target_goal']].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore ratings\n",
    "print('=== Ratings ===')\n",
    "print(f'\\nRating Distribution:')\n",
    "print(ratings['rating'].value_counts().sort_index())\n",
    "print(f'\\nMean rating: {ratings[\"rating\"].mean():.2f}')\n",
    "print(f'Ratings per user  ‚Äî min: {ratings.groupby(\"user_id\").size().min()}, '\n",
    "      f'max: {ratings.groupby(\"user_id\").size().max()}, '\n",
    "      f'mean: {ratings.groupby(\"user_id\").size().mean():.1f}')\n",
    "print(f'Ratings per plan  ‚Äî min: {ratings.groupby(\"plan_id\").size().min()}, '\n",
    "      f'max: {ratings.groupby(\"plan_id\").size().max()}, '\n",
    "      f'mean: {ratings.groupby(\"plan_id\").size().mean():.1f}')\n",
    "\n",
    "ratings.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize distributions\n",
    "fig, axes = plt.subplots(1, 4, figsize=(18, 4))\n",
    "\n",
    "# Rating distribution\n",
    "ratings['rating'].value_counts().sort_index().plot(kind='bar', ax=axes[0], color='#667eea')\n",
    "axes[0].set_title('Rating Distribution')\n",
    "axes[0].set_xlabel('Rating')\n",
    "axes[0].set_ylabel('Count')\n",
    "\n",
    "# Fitness level\n",
    "users['fitness_level'].value_counts().plot(kind='bar', ax=axes[1], color='#764ba2')\n",
    "axes[1].set_title('Fitness Level Distribution')\n",
    "axes[1].set_xlabel('Level')\n",
    "\n",
    "# Goal distribution\n",
    "users['goal'].value_counts().plot(kind='bar', ax=axes[2], color='#f093fb')\n",
    "axes[2].set_title('User Goal Distribution')\n",
    "axes[2].set_xlabel('Goal')\n",
    "\n",
    "# Plans per type\n",
    "plans['type'].value_counts().plot(kind='bar', ax=axes[3], color='#4facfe')\n",
    "axes[3].set_title('Plans by Type')\n",
    "axes[3].set_xlabel('Type')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/Test Split (80/20)\n",
    "train, test = split_train_test(ratings, test_size=0.2, seed=42)\n",
    "\n",
    "print(f'Train set: {len(train)} ratings')\n",
    "print(f'Test set:  {len(test)} ratings')\n",
    "print(f'Ratio:     {len(train)/len(ratings):.0%} / {len(test)/len(ratings):.0%}')\n",
    "print(f'\\nUsers in train: {train[\"user_id\"].nunique()}')\n",
    "print(f'Users in test:  {test[\"user_id\"].nunique()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 3: Model Development"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Most Popular Recommender (Baseline)\n",
    "pop_model = MostPopularRecommender()\n",
    "pop_model.fit(train)\n",
    "\n",
    "print('=== Most Popular Baseline ===')\n",
    "print('Top 10 most popular workout plans:')\n",
    "top_popular = pop_model.popular_plans.head(10).merge(\n",
    "    plans[['plan_id', 'name', 'type']], on='plan_id'\n",
    ")\n",
    "top_popular[['plan_id', 'name', 'type', 'mean', 'count']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. User-Based Collaborative Filtering\n",
    "cf_model = UserBasedCFRecommender(k_neighbors=20)\n",
    "cf_model.fit(train)\n",
    "\n",
    "print('=== User-Based CF ===')\n",
    "print(f'User-Item matrix shape: {cf_model.user_item_matrix.shape}')\n",
    "print(f'Similarity matrix shape: {cf_model.similarity_matrix.shape}')\n",
    "\n",
    "# Show similarity for first user\n",
    "user1 = cf_model.user_ids[0]\n",
    "top_similar = cf_model.similarity_matrix[user1].drop(user1).nlargest(5)\n",
    "print(f'\\nTop 5 similar users to User #{user1}:')\n",
    "for uid, sim in top_similar.items():\n",
    "    print(f'  User #{uid}: similarity = {sim:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Content-Based Recommender (TF-IDF)\n",
    "cb_model = ContentBasedRecommender()\n",
    "cb_model.fit(plans, train)\n",
    "\n",
    "print('=== Content-Based (TF-IDF) ===')\n",
    "print(f'TF-IDF matrix shape: {cb_model.tfidf_matrix.shape}')\n",
    "\n",
    "# Show plan similarity example\n",
    "print(f'\\nMost similar plans to \"{plans.iloc[0][\"name\"]}\":')\n",
    "sim_scores = cb_model.similarity_matrix[1].drop(1).nlargest(5)\n",
    "for pid, sim in sim_scores.items():\n",
    "    plan_name = plans[plans['plan_id'] == pid]['name'].values[0]\n",
    "    print(f'  Plan #{pid} ({plan_name}): similarity = {sim:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Hybrid Recommender (CF + Content-Based)\n",
    "hybrid_model = HybridRecommender(cf_weight=0.6, cb_weight=0.4)\n",
    "hybrid_model.fit(train, plans)\n",
    "\n",
    "print('=== Hybrid Recommender ===')\n",
    "print('Weights: CF = 0.6, Content-Based = 0.4')\n",
    "print('Model trained successfully!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate sample recommendations from all models for User #1\n",
    "sample_uid = 1\n",
    "user_info = users[users['user_id'] == sample_uid].iloc[0]\n",
    "print(f'Sample User #{sample_uid}: Age={user_info[\"age\"]}, '\n",
    "      f'Fitness={user_info[\"fitness_level\"]}, Goal={user_info[\"goal\"]}')\n",
    "print('=' * 70)\n",
    "\n",
    "models = {\n",
    "    'Most Popular': pop_model,\n",
    "    'User-Based CF': cf_model,\n",
    "    'Content-Based': cb_model,\n",
    "    'Hybrid': hybrid_model\n",
    "}\n",
    "\n",
    "for name, model in models.items():\n",
    "    rec_ids = model.recommend(sample_uid, train, n=5)\n",
    "    rec_plans = plans[plans['plan_id'].isin(rec_ids)].set_index('plan_id').loc[rec_ids].reset_index()\n",
    "    print(f'\\n--- {name} ---')\n",
    "    for _, row in rec_plans.iterrows():\n",
    "        print(f'  ‚Ä¢ {row[\"name\"]} ({row[\"type\"]}, {row[\"difficulty\"]}, '\n",
    "              f'{row[\"duration_min\"]}min, goal={row[\"target_goal\"]})')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 4: Evaluation\n",
    "\n",
    "We evaluate all models using **Precision@5** and **Recall@5** on the held-out test set.\n",
    "\n",
    "- **Precision@5** = fraction of the 5 recommended items that appear in the user's relevant test items (rated ‚â• 4)\n",
    "- **Recall@5** = fraction of the user's relevant test items that appear in the top-5 recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating models (this may take a moment)...\n",
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'models' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEvaluating models (this may take a moment)...\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      4\u001b[0m results \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, model \u001b[38;5;129;01min\u001b[39;00m \u001b[43mmodels\u001b[49m\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m      6\u001b[0m     metrics \u001b[38;5;241m=\u001b[39m evaluate_model(model, train, test, k\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, threshold\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m)\n\u001b[0;32m      7\u001b[0m     results[name] \u001b[38;5;241m=\u001b[39m metrics\n",
      "\u001b[1;31mNameError\u001b[0m: name 'models' is not defined"
     ]
    }
   ],
   "source": [
    "# Evaluate all models\n",
    "print('Evaluating models (this may take a moment)...\\n')\n",
    "\n",
    "results = {}\n",
    "for name, model in models.items():\n",
    "    metrics = evaluate_model(model, train, test, k=5, threshold=4)\n",
    "    results[name] = metrics\n",
    "    print(f'{name:20s}  Precision@5={metrics[\"Precision@5\"]:.4f}  '\n",
    "          f'Recall@5={metrics[\"Recall@5\"]:.4f}  '\n",
    "          f'(evaluated on {metrics[\"Users Evaluated\"]} users)')\n",
    "\n",
    "# Comparison table\n",
    "print('\\n' + '=' * 70)\n",
    "print('PERFORMANCE COMPARISON TABLE')\n",
    "print('=' * 70)\n",
    "comparison = pd.DataFrame(results).T\n",
    "comparison.index.name = 'Model'\n",
    "comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize comparison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "model_names = list(results.keys())\n",
    "precisions = [results[m]['Precision@5'] for m in model_names]\n",
    "recalls = [results[m]['Recall@5'] for m in model_names]\n",
    "colors = ['#667eea', '#764ba2', '#f093fb', '#4facfe']\n",
    "\n",
    "axes[0].barh(model_names, precisions, color=colors)\n",
    "axes[0].set_title('Precision@5 Comparison', fontweight='bold')\n",
    "axes[0].set_xlabel('Precision@5')\n",
    "for i, v in enumerate(precisions):\n",
    "    axes[0].text(v + 0.002, i, f'{v:.4f}', va='center')\n",
    "\n",
    "axes[1].barh(model_names, recalls, color=colors)\n",
    "axes[1].set_title('Recall@5 Comparison', fontweight='bold')\n",
    "axes[1].set_xlabel('Recall@5')\n",
    "for i, v in enumerate(recalls):\n",
    "    axes[1].text(v + 0.002, i, f'{v:.4f}', va='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 5: Demonstration\n",
    "\n",
    "Below we show detailed recommendations for **3 different users** with different fitness profiles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select 3 diverse demo users\n",
    "demo_user_ids = []\n",
    "for goal in ['weight_loss', 'muscle_gain', 'endurance']:\n",
    "    uid = users[users['goal'] == goal].iloc[0]['user_id']\n",
    "    demo_user_ids.append(uid)\n",
    "\n",
    "for uid in demo_user_ids:\n",
    "    user = users[users['user_id'] == uid].iloc[0]\n",
    "    user_train_ratings = train[train['user_id'] == uid]\n",
    "    \n",
    "    print('=' * 70)\n",
    "    print(f'üë§ USER #{uid}')\n",
    "    print(f'   Age: {user[\"age\"]}  |  Fitness: {user[\"fitness_level\"]}  |  Goal: {user[\"goal\"]}')\n",
    "    print(f'   Training ratings: {len(user_train_ratings)}')\n",
    "    print('-' * 70)\n",
    "    \n",
    "    # Show what they rated in training\n",
    "    rated_plans = user_train_ratings.merge(plans[['plan_id', 'name', 'type']], on='plan_id')\n",
    "    print('\\n   üìù Plans they rated (training set):')\n",
    "    for _, r in rated_plans.iterrows():\n",
    "        stars = '‚≠ê' * r['rating']\n",
    "        print(f'      {r[\"name\"]:35s} ({r[\"type\"]:15s}) ‚Üí {stars}')\n",
    "    \n",
    "    # Show CF recommendations\n",
    "    print('\\n   ü§ñ Top 5 User-Based CF Recommendations:')\n",
    "    cf_recs = cf_model.recommend(uid, train, n=5)\n",
    "    for i, pid in enumerate(cf_recs, 1):\n",
    "        p = plans[plans['plan_id'] == pid].iloc[0]\n",
    "        print(f'      {i}. {p[\"name\"]:35s} | {p[\"type\"]:15s} | '\n",
    "              f'{p[\"difficulty\"]:12s} | {p[\"duration_min\"]}min | üéØ {p[\"target_goal\"]}')\n",
    "    \n",
    "    # Brief explanation\n",
    "    print(f'\\n   üí° Explanation: These plans are recommended because users with similar')\n",
    "    print(f'      rating patterns (who also enjoy {user[\"goal\"].replace(\"_\", \" \")} workouts')\n",
    "    print(f'      at the {user[\"fitness_level\"]} level) rated these plans highly.')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final comparison summary table\n",
    "print('\\n' + '=' * 70)\n",
    "print('FINAL PERFORMANCE COMPARISON')\n",
    "print('=' * 70)\n",
    "\n",
    "summary = pd.DataFrame({\n",
    "    'Model': ['Most Popular (Baseline)', 'User-Based CF (Primary)', \n",
    "              'Content-Based TF-IDF (Bonus)', 'Hybrid CF+CB (Bonus)'],\n",
    "    'Precision@5': [results['Most Popular']['Precision@5'],\n",
    "                    results['User-Based CF']['Precision@5'],\n",
    "                    results['Content-Based']['Precision@5'],\n",
    "                    results['Hybrid']['Precision@5']],\n",
    "    'Recall@5': [results['Most Popular']['Recall@5'],\n",
    "                 results['User-Based CF']['Recall@5'],\n",
    "                 results['Content-Based']['Recall@5'],\n",
    "                 results['Hybrid']['Recall@5']],\n",
    "})\n",
    "\n",
    "print(summary.to_string(index=False))\n",
    "\n",
    "print('\\n\\nConclusion:')\n",
    "print('The User-Based Collaborative Filtering model outperforms the Most Popular')\n",
    "print('baseline by providing personalized recommendations. The Hybrid model')\n",
    "print('combines the strengths of both CF and Content-Based approaches.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
